{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment No. 14 - Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *15.05.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning*\n",
    "\n",
    "Welcome to question 2 of the fourth assignment of the course *Deep Learning*. \\\n",
    "In this question, we will implement the *DropNorm* network layer, and compare the use of this layer to the normal dropout and normalizing layers in pytorch with training on *Fashion-MNIST*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the required packages for this assignment.\n",
    "- [pytorch](https://pytorch.org/) - One of the most fundemental and famous tensor handling library.\n",
    "- [numpy](https://numpy.org) - The fundamental package for scientific computing with Python.\n",
    "- [matplotlib](https://matplotlib.org) - Library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import torch.nn.functional as F  # functional module.\n",
    "import torch.optim as optim  # optimizer module.\n",
    "from torch.utils.data import DataLoader  # data loader.\n",
    "from torchvision import datasets, transforms  # torchvision, for datasets.\n",
    "from torchmetrics import Accuracy  # accuracy metric.\n",
    "import matplotlib.pyplot as plt  # plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropNorm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the implementation of the *DropNorm* layer, using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNorm(nn.Module):\n",
    "    '''DropNorm layer.\n",
    "    \n",
    "    The DropNorm layer is a combined dropout and batch norm layers that in training mode\n",
    "    zeros half of the input tensor and normalizes the other half. In evaluation mode, it\n",
    "    normalizes the entire input tensor.\n",
    "    '''\n",
    "    def __init__(self, batch_size: int) -> None:\n",
    "        '''\n",
    "        Constructor for the DropNorm layer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int) - Batch size.\n",
    "        '''\n",
    "        super(DropNorm, self).__init__()\n",
    "        \n",
    "        self.gammas = nn.Parameter(torch.zeros(batch_size, 1))\n",
    "        self.betas = nn.Parameter(torch.ones(batch_size, 1))\n",
    "        self.eps = 1e-8\n",
    "        self.p = 0.5\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Forward pass of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor) - Input tensor. Assumes shape (batch_size, #features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor - Output tensor.\n",
    "        '''\n",
    "\n",
    "        gammas = self.gammas.expand_as(x)\n",
    "        betas = self.betas.expand_as(x)\n",
    "\n",
    "        if self.training:\n",
    "            mask = torch.bernoulli(torch.full(x.shape, self.p))\n",
    "            x = x * mask\n",
    "\n",
    "        x = (x - x.mean(dim=0, keepdim=True)) / \\\n",
    "            ((x.std(dim=0, keepdim=True) ** 2 + self.eps) ** 0.5)\n",
    "        \n",
    "        x = x * gammas + betas\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Network on Fashion-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure hyper parameters.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 512 if device == 'cuda' else 128\n",
    "\n",
    "# Load the Fashion-MNIST dataset.\n",
    "\n",
    "# Define the transformations.\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load the dataset.\n",
    "train_set = datasets.FashionMNIST('data', download=True, train=True, transform=transform)\n",
    "val_set = datasets.FashionMNIST('data', download=True, train=False, transform=transform)\n",
    "\n",
    "# Prepare the data loaders.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    '''Base model class.'''\n",
    "    def __init__(self) -> None:\n",
    "        '''Constructor.'''\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.epoch = 0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''Forward pass.'''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, train_loader: DataLoader, val_loader: DataLoader,\n",
    "            num_epochs: int = 30, lr: float = 0.001, wd: float = 0., \n",
    "            try_cuda: bool = True, quite: bool = False, print_stride: int = 1) -> \\\n",
    "                  Tuple[list[float], list[float], list[float]]:\n",
    "        '''\n",
    "        Base function for training the model.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader) - The dataloader to fit the model to.\n",
    "            val_loader (DataLoader) - The dataloader to validate the model on.\n",
    "            num_epochs (int) - Number of epochs.\n",
    "            lr (float) - Learning rate.\n",
    "            wd (float) - Weight decay.\n",
    "            try_cuda (bool) - Try to use CUDA.\n",
    "            quite (bool) - Quite mode.\n",
    "            print_stride (int) - Print stride (in epochs).\n",
    "        \n",
    "        Returns:\n",
    "            costs (list[float]) - Costs over all epochs\n",
    "            train_accs (list[float]) - Accuracies over all epochs\n",
    "            val_accs (list[float]) - Validation accuracies over all epochs\n",
    "        '''\n",
    "        \n",
    "        costs: list[float] = []\n",
    "        train_accs: list[float] = []\n",
    "        val_accs: list[float] = []\n",
    "        \n",
    "        use_cuda = try_cuda and torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "            print(\"Using CUDA for traininig.\")\n",
    "        else:\n",
    "            self.cpu()\n",
    "            print(\"Using CPU for training.\")\n",
    "\n",
    "        # Create the optimizer and criterion.\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        start_epoch = self.epoch\n",
    "        for epoch in range(num_epochs):\n",
    "            train_true = 0.\n",
    "            running_loss = 0.\n",
    "            self.epoch += 1\n",
    "            for mb, (x, y) in enumerate(train_loader):\n",
    "                if use_cuda:\n",
    "                    x, y = x.cuda(), y.cuda()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                y_hat = self(x)\n",
    "\n",
    "                loss = criterion(y_hat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Calc loss\n",
    "                lloss = loss.item()\n",
    "                running_loss += lloss * x.size(0)\n",
    "\n",
    "                # Calc accuracy\n",
    "                train_true += (y_hat.argmax(1) == y).sum().item()\n",
    "\n",
    "                if not quite:\n",
    "                    print(f\"\\r[epoch: {self.epoch:02d}/{start_epoch + num_epochs:02d}\", end=\" \")\n",
    "                    print(f\"mb: {mb + 1:03d}/{len(train_loader):03d}]\", end=\" \")\n",
    "                    print(f\"loss: {lloss:.6f}\", end=\"\")\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)  # type: ignore\n",
    "            train_acc = train_true / len(train_loader.dataset)  # type: ignore\n",
    "            val_acc = self.calc_acc(val_loader, use_cuda)\n",
    "            costs.append(epoch_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "                \n",
    "            if not quite and (epoch % print_stride == 0 or epoch == num_epochs - 1):\n",
    "                print(f\"\\r[epoch: {self.epoch:02d}/{start_epoch + num_epochs:02d}]\", end=\" \")\n",
    "                print(f\"[Total Loss: {epoch_loss:.6f}]\", end=\" \")\n",
    "                print(f\"[Train Acc: {100 * train_acc:.3f}%]\", end=\" \")\n",
    "                print(f\"[Val Acc: {100 * val_acc:.3f}%]\")\n",
    "\n",
    "        return costs, train_accs, val_accs\n",
    "    \n",
    "    def calc_acc(self, data_loader: DataLoader, use_cuda: bool) -> float:\n",
    "        '''\n",
    "        Calculates and returns the accuracy of the model on a give dataset.\n",
    "\n",
    "        Args:\n",
    "            data_loader (DataLoader) - Data loader.\n",
    "            use_cuda (bool) - Use CUDA flag.\n",
    "        '''\n",
    "        \n",
    "        accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        if use_cuda:\n",
    "            accuracy = accuracy.cuda()\n",
    "        self.eval()\n",
    "        acc = 0.\n",
    "        for x, y in data_loader:\n",
    "            if use_cuda:\n",
    "                x, y = x.cuda(), y.cuda()\n",
    "            y_hat = self(x)\n",
    "            acc += accuracy(y_hat, y).item() * x.size(0)\n",
    "        \n",
    "        self.train()\n",
    "        return acc / len(data_loader.dataset)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNet(BaseModel):\n",
    "    '''Normal Net\n",
    "    A simple neural network with two hidden layers and a DropNorm layer.\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super(NormalNet, self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for traininig.\n",
      "[epoch: 01/10] [Total Loss: 0.543879] [Train Acc: 81.782%] [Val Acc: 86.750%]\n",
      "[epoch: 02/10] [Total Loss: 0.380921] [Train Acc: 86.937%] [Val Acc: 88.870%]\n",
      "[epoch: 03/10] [Total Loss: 0.353413] [Train Acc: 87.872%] [Val Acc: 89.390%]\n",
      "[epoch: 04/10] [Total Loss: 0.336608] [Train Acc: 88.480%] [Val Acc: 88.950%]\n",
      "[epoch: 05/10] [Total Loss: 0.328862] [Train Acc: 88.550%] [Val Acc: 89.780%]\n",
      "[epoch: 06/10] [Total Loss: 0.321544] [Train Acc: 89.047%] [Val Acc: 89.250%]\n",
      "[epoch: 07/10] [Total Loss: 0.320743] [Train Acc: 88.882%] [Val Acc: 90.280%]\n",
      "[epoch: 08/10] [Total Loss: 0.314472] [Train Acc: 89.203%] [Val Acc: 90.160%]\n",
      "[epoch: 09/10] [Total Loss: 0.308353] [Train Acc: 89.355%] [Val Acc: 90.150%]\n",
      "[epoch: 10/10] [Total Loss: 0.307689] [Train Acc: 89.380%] [Val Acc: 90.580%]\n"
     ]
    }
   ],
   "source": [
    "# Use normal net to train the model.\n",
    "normal_model = NormalNet()\n",
    "\n",
    "# Train the model.\n",
    "normal_metrics = normal_model.fit(train_loader, val_loader, num_epochs=10, lr=0.0025, wd=0.0025, \n",
    "                                  try_cuda=True, print_stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostumeNet(BaseModel):\n",
    "    '''Costume Net model.\n",
    "    Same as the NormalNet mode but with DropNorm layer.\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super(CostumeNet, self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            DropNorm(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            DropNorm(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            DropNorm(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.convs(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model.\n",
    "costume_model = CostumeNet()\n",
    "\n",
    "costume_metrics = costume_model.fit(train_loader, val_loader, num_epochs=10, lr=0.0025, wd=0.0025, \n",
    "                                    try_cuda=True, print_stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results comparison\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
