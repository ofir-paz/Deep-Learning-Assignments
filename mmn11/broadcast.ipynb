{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment No. 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *14.03.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning*\n",
    "\n",
    "Welcome to the first assignment of the course *Deep Learning*. \\\n",
    "In this first assignemnt we will implement:\n",
    "\n",
    "$\\quad$ [**a.**](#a) $\\space$ `A.expand_as(B)` functionality. \\\n",
    "$\\quad$ [**b.**](#b) $\\space$ A function that tests if two tensors can be broadcasted together, and return the size of the broadcast. \\\n",
    "$\\quad$ [**c.**](#c) $\\space$ A function which broadcasts two tensors. \\\n",
    "$\\quad$ [**d.**](#d) $\\space$ Tests for the different implemented functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowed functions in this assignment:\n",
    "\n",
    "$\\qquad$ `squeeze` `unsqueeze` `cat` `stack` `x.reshape` `x.reshape_as` `x.clone`\n",
    "\n",
    "Unallowed functions in this assignment:\n",
    "\n",
    "$\\qquad$ `x.expand` `x.expand_as` `x.repeat` `broadcast_tensors` `broadcast_to` `vmap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the required packages for this assignment\n",
    "- [pytorch](https://pytorch.org/) - One of the most fundemental and famous tensor handling library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A <a id='a'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I implemented `expand_as`, which is a function that takes two tensors and checks if the first tensor is expandable to the second tensor, and if it is, expands it. This while not altering the dimentions of the second tensor whatsoever. We first start with the help method, `is_legal_expand`, which returns `True` or `False` if the first tensor can be expanded to the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legal_expand(a_shape: torch.Size, b_shape: torch.Size) -> bool:\n",
    "  \"\"\"Check if the shape of `a` can be expanded to the shape of `b`.\n",
    "  \n",
    "  The laws of tensor expansion are as follows:\n",
    "  \n",
    "  a.  Start from the last dimension (the rightmost) of both tensors and check if:\n",
    "      They are equal to each other, or the current dimension of `a` is equal to 1.\n",
    "      If none of these conditions are met, return `False`.\n",
    "\n",
    "  b.  Move one dimension to the left in each of the tensors and repeat the above check.\n",
    "\n",
    "  c.  Once we have iterated over all dimensions from right to left in at least one of the tensors\n",
    "      without encountering any error, return `True`.\n",
    "\n",
    "  Args:\n",
    "    a_shape: shape of tensor a.\n",
    "    b_shape: shape of tensor b.\n",
    "\n",
    "  Returns:\n",
    "    True if `a` can be expanded to `b`, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  # reverse the shapes of tensors.\n",
    "  a_shape = a_shape[::-1]\n",
    "  b_shape = b_shape[::-1]\n",
    "\n",
    "  # Check if there are enough dimensions in `b` to expand `a` to.\n",
    "  if len(a_shape) > len(b_shape):\n",
    "    return False\n",
    "\n",
    "  # iterate over the shapes of tensors.\n",
    "  for a_dim, b_dim in zip(a_shape, b_shape):\n",
    "    if a_dim != b_dim and a_dim != 1:\n",
    "      return False\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_as(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor | bool:\n",
    "  \"\"\"Expand `tensor_a` to the same size as `tensor_b`\n",
    "  \n",
    "  This is my implementation of the expand_as function.\n",
    "  If `tensor_a` can't be expanded to `tensor_b`, the function returns False.\n",
    "\n",
    "  During the expansion stage, we duplicate the tensor values according to the following rules:\n",
    "\n",
    "  a.  If the tensor has smaller dimensions than the other (i.e., fewer dimensions),\n",
    "      we prepend singleton dimensions to it until the number of dimensions in both \n",
    "      tensors is equal.\n",
    "\n",
    "  b.  Whenever there's a mismatch in dimensionality, the expansion compatibility\n",
    "      check succeeds. Therefore, `tensor_a` has a singleton dimension of size 1,\n",
    "      which is expanded along that dimension.\n",
    "\n",
    "  Args:\n",
    "    tensor_a (torch.Tensor): tensor to expand.\n",
    "    tensor_b (torch.Tensor): tensor to expand to.\n",
    "\n",
    "  Returns:\n",
    "    torch.Tensor | False: `tensor_a` expanded to the same size as `tensor_b` or\n",
    "    `False` if `tensor_a` can't be expanded to `tensor_b`.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Extracts the tensors' shapes.\n",
    "  a_shape, b_shape = tensor_a.shape, tensor_b.shape \n",
    "\n",
    "  # Return `False` if tensor_a can't be expanded to `tensor_b`.\n",
    "  if not is_legal_expand(a_shape, b_shape):\n",
    "    return False\n",
    "  \n",
    "  # expand `tensor_a` to the same size as `tensor_b`.\n",
    "  # Stage 1: Prepend singleton dimensions to `tensor_a`.\n",
    "  for _ in range(len(b_shape) - len(a_shape)):\n",
    "    tensor_a = tensor_a.unsqueeze(0) \n",
    "  \n",
    "  a_shape = tensor_a.shape  # update `a_shape` after prepending singleton dimensions.\n",
    "\n",
    "  # Stage 2: expand `tensor_a` to `tensor_b` while assuming it can be expanded.\n",
    "  for idx_dim, (a_dim, b_dim) in enumerate(zip(a_shape, b_shape)) :\n",
    "    if a_dim != b_dim:  # This means that `a_dim` is 1, and `b_dim` is not 1.\n",
    "      tensor_a = torch.cat([tensor_a] * b_dim, dim=idx_dim)  # duplicate `tensor_a` values.\n",
    "\n",
    "  return tensor_a\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B <a id='b'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part requires us to create a function that checks if two tensors can be broadcasted togther, and if so, return the size of the broadcast. We will create two separate function like we did in [Part A](#a) to achieve this goal. The code will be similar to the code from the last part, but we cannot use the functions from there since the broadcasting operation is different from the expansion operation because it affects both participating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_legal_broadcast(a_shape: torch.Size, b_shape: torch.Size) -> bool:\n",
    "  \"\"\"Check if `a` and `b` can be broadcasted together based on their shapes.\n",
    "  \n",
    "  The laws of tensor broadcast are as follows:\n",
    "  \n",
    "  a.  Start from the last dimension (the rightmost) of both tensors and check if:\n",
    "      They are equal to each other, or the current dimension of any tensor is equal to 1.\n",
    "      If none of these conditions are met, return `False`.\n",
    "\n",
    "  b.  Move one dimension to the left in each of the tensors and repeat the above check.\n",
    "\n",
    "  c.  Once we have iterated over all dimensions from right to left in at least one of the tensors\n",
    "      without encountering any error, return `True`.\n",
    "\n",
    "  Args:\n",
    "    a_shape: shape of tensor a.\n",
    "    b_shape: shape of tensor b.\n",
    "\n",
    "  Returns:\n",
    "    True if `a` can be expanded to `b`, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  # reverse the shapes of tensors.\n",
    "  a_shape = a_shape[::-1]\n",
    "  b_shape = b_shape[::-1]\n",
    "\n",
    "  # iterate over the shapes of tensors and return `False` if the condition is not met.\n",
    "  for a_dim, b_dim in zip(a_shape, b_shape):\n",
    "    if a_dim != b_dim and a_dim != 1 and b_dim != 1:\n",
    "      return False\n",
    "\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_broadcast_info(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> \\\n",
    "    tuple[torch.Size | None, bool]:\n",
    "  \"\"\"Get the broadcasting information between `tensor_a` and `tensor_b`.\n",
    "  \n",
    "  Args:\n",
    "    tensor_a (torch.Tensor): first tensor of the broadcasting operation. \n",
    "    tensor_b (torch.Tensor): second tensor of the broadcasting operation.\n",
    "\n",
    "  Returns:\n",
    "    tuple[torch.Size | None, bool]: A tuple containing the following information:\n",
    "    - The shape of the broadcasted tensor if the first element is True, and None otherwise.\n",
    "    - A boolean value indicating whether `tensor_a` can be broadcasted with `tensor_b`.\n",
    "  \"\"\"\n",
    "\n",
    "  # Extracts the tensors' shapes.\n",
    "  a_shape, b_shape = tensor_a.shape, tensor_b.shape\n",
    "\n",
    "  # Return `False` if `tensor_a` can't be broadcasted with `tensor_b`.\n",
    "  if not is_legal_broadcast(a_shape, b_shape):\n",
    "    return None, False\n",
    "\n",
    "  # Add singleton dimensions to smaller shape.\n",
    "  for _ in range(abs(len(b_shape) - len(a_shape))):\n",
    "    if len(a_shape) < len(b_shape):\n",
    "      a_shape = (1,) + a_shape\n",
    "    else:\n",
    "      b_shape = (1,) + b_shape\n",
    "  \n",
    "  # Initialize the broadcast shape.\n",
    "  broadcast_shape = []\n",
    "\n",
    "  # Iterate over the shapes of tensors.\n",
    "  for a_dim, b_dim in zip(a_shape[::-1], b_shape[::-1]):\n",
    "    broadcast_shape.append(max(a_dim, b_dim))  # get the maximum value between `a_dim` and `b_dim`.\n",
    "  \n",
    "  return torch.Size(broadcast_shape[::-1]), True  # return the broadcast shape and `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C <a id='c'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-section, we will combine the functions from the previous part to make the general broadcasting function. This function will take two tensors, check if the tensors can be broadcasted, and if so broadcast them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_tensors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> \\\n",
    "    tuple[torch.Tensor, torch.Tensor] | None:\n",
    "  \"\"\"Broadcast `tensor_a` and `tensor_b` together.\n",
    "\n",
    "  This is my implementation of the broadcast_tensors function.\n",
    "  If `tensor_a` can't be broadcasted with `tensor_b`, the function will not do anything\n",
    "  and return `None`.\n",
    "\n",
    "  Args:\n",
    "    tensor_a (torch.Tensor): first tensor of the broadcasting operation. \n",
    "    tensor_b (torch.Tensor): second tensor of the broadcasting operation.\n",
    "  \n",
    "  Returns:\n",
    "    tuple[torch.Tensor, torch.Tensor] | None: A tuple containing the following information:\n",
    "    - The broadcasted `tensor_a` and `tensor_b` if the tensors can be broadcasted, and None otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the info about the broadcast.\n",
    "  broadcast_shape, is_legal_broadcast = get_broadcast_info(tensor_a, tensor_b)\n",
    "\n",
    "  # Check if the two tensors can be broadcased together.\n",
    "  if not is_legal_broadcast:\n",
    "    return None\n",
    "  \n",
    "  # Broadcast `tensor_a` and `tensor_b` together.\n",
    "  # Extract the current shapes of the tensors.\n",
    "  a_shape, b_shape = tensor_a.shape, tensor_b.shape\n",
    "\n",
    "  # Copy the tensors to avoid modifying the original tensors.\n",
    "  tensor_a, tensor_b = tensor_a.clone(), tensor_b.clone()\n",
    "\n",
    "  # Stage 1: Prepend singleton dimensions to the smaller tensor.\n",
    "  for _ in range(abs(len(b_shape) - len(a_shape))):\n",
    "    if len(a_shape) < len(b_shape):\n",
    "      tensor_a = tensor_a.unsqueeze(0)\n",
    "    else:  # len(a_shape) > len(b_shape)\n",
    "      tensor_b = tensor_b.unsqueeze(0)\n",
    "  \n",
    "  # update shapes after prepending singleton dimensions.\n",
    "  a_shape, b_shape = tensor_a.shape, tensor_b.shape\n",
    "\n",
    "  # Stage 2: broadcast `tensor_a` with `tensor_b` while assuming they can be broadcasted.\n",
    "  for idx_dim, (a_dim, b_dim, bc_dim) in enumerate(zip(a_shape, b_shape, broadcast_shape)):\n",
    "    if a_dim != bc_dim:  # Broadcast dimention of `tensor_a` if needed.\n",
    "      tensor_a = torch.cat([tensor_a] * bc_dim, dim=idx_dim)  # duplicate `tensor_a` values.\n",
    "\n",
    "    if b_dim != bc_dim:  # Broadcast dimention of `tensor_b` if needed.\n",
    "      tensor_b = torch.cat([tensor_b] * bc_dim, dim=idx_dim)  # duplicate `tensor_b` values.\n",
    "  \n",
    "  return tensor_a, tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D <a id='d'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-section, we will implement tests for the various self-created functions.\n",
    "We will time the tests and check if the outputs are correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block contains tests for [Part A](#a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand_as(a, c) = False\n",
      "expand_as(b, a) = False\n",
      "Test Passed!\n",
      "\n",
      "expand_as(a, b).shape = torch.Size([100, 30, 2, 82, 7, 10])\n",
      "expand_as(c, a).shape = torch.Size([1, 2, 82, 1, 10])\n",
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# `expand_as` correctness tests.\n",
    "a = torch.zeros(1, 2, 82, 1, 10)\n",
    "b = torch.zeros(100, 30, 2, 82, 7, 10)\n",
    "c = torch.zeros(1, 82, 1, 10)\n",
    "\n",
    "# Inputs for `False` expected output.\n",
    "print(f\"{expand_as(a, c) = }\", f\"{expand_as(b, a) = }\", sep='\\n')  # expected output: False.\n",
    "print('Test Passed!' if expand_as(a, c) is False and expand_as(b, a) is False else 'Test Failed!')\n",
    "print()  # Next test.\n",
    "\n",
    "print(f\"{expand_as(a, b).shape = }\", f\"{expand_as(c, a).shape = }\", sep='\\n')  # expected output: Shapes.\n",
    "print('Test Passed!' if expand_as(a, b).shape == a.expand_as(b).shape and\n",
    "          expand_as(c, a).shape == c.expand_as(a).shape else 'Test Failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block contains tests for [Part B](#b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_broadcast_info(d, c) = (None, False)\n",
      "get_broadcast_info(c, a) = (None, False)\n",
      "Test Passed!\n",
      "\n",
      "get_broadcast_info(a, b) = (torch.Size([100, 30, 2, 82, 7, 10]), True)\n",
      "get_broadcast_info(b, c) = (torch.Size([100, 30, 2, 82, 7, 5]), True)\n",
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# `get_broadcast_info` correctness tests.\n",
    "a = torch.zeros(1, 2, 82, 1, 10)\n",
    "b = torch.zeros(100, 30, 1, 82, 7, 1)\n",
    "c = torch.zeros(2, 82, 1, 5)\n",
    "d = torch.zeros(2, 8, 1)\n",
    "\n",
    "# Inputs for `None`, `False` expected output.\n",
    "print(f\"{get_broadcast_info(d, c) = }\", f\"{get_broadcast_info(c, a) = }\",\n",
    "       sep='\\n')  # expected output: `None`, `False`.\n",
    "print('Test Passed!' if get_broadcast_info(d, c) == (None, False) and\n",
    "          get_broadcast_info(c, a) == (None, False) else 'Test Failed!')\n",
    "print()  # Next test.\n",
    "\n",
    "print(f\"{get_broadcast_info(a, b) = }\", f\"{get_broadcast_info(b, c) = }\",\n",
    "        sep='\\n')  # expected output: Shapes.\n",
    "print('Test Passed!' if get_broadcast_info(a, b)[0] == torch.broadcast_tensors(a, b)[0].shape and\n",
    "          get_broadcast_info(b, c)[0] == torch.broadcast_tensors(b, c)[0].shape else 'Test Failed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block contains tests for [Part C](#c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcast_tensors(d, c) = None\n",
      "broadcast_tensors(c, a) = None\n",
      "Test Passed!\n",
      "\n",
      "broadcast_tensors(a, b)[0].shape = torch.Size([100, 30, 2, 82, 7, 10])\n",
      "broadcast_tensors(b, c)[0].shape = torch.Size([100, 30, 2, 82, 7, 5])\n",
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# `broadcast_tensors` correctness tests.\n",
    "a = torch.zeros(1, 2, 82, 1, 10)\n",
    "b = torch.zeros(100, 30, 1, 82, 7, 1)\n",
    "c = torch.zeros(2, 82, 1, 5)\n",
    "d = torch.zeros(2, 8, 1)\n",
    "\n",
    "# Inputs for `None` expected output.\n",
    "print(f\"{broadcast_tensors(d, c) = }\", f\"{broadcast_tensors(c, a) = }\",\n",
    "       sep='\\n')  # expected output: `None`.\n",
    "print('Test Passed!' if broadcast_tensors(d, c) is None and\n",
    "          broadcast_tensors(c, a) is None else 'Test Failed!')\n",
    "print()  # Next test.\n",
    "\n",
    "print(f\"{broadcast_tensors(a, b)[0].shape = }\", f\"{broadcast_tensors(b, c)[0].shape = }\",\n",
    "        sep='\\n')  # expected output: Shapes.\n",
    "print('Test Passed!' if (broadcast_tensors(a, b)[0] == torch.broadcast_tensors(a, b)[0]).all() and\n",
    "          (broadcast_tensors(b, c)[0] == torch.broadcast_tensors(b, c)[0]).all() else 'Test Failed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
