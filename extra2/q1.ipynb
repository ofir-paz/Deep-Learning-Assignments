{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment - Extra Credit #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *17.07.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning* \\\n",
    "***Extra Assignment Course:*** *20999 - Extra Assignment 4*\n",
    "\n",
    "Welcome to the first question of the extra assignment #2 as part of the course *Deep Learning*. \\\n",
    "In this question we will train different types of auto encoders on the MNIST dataset and compare the results with various plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import numpy as np  # numpy - scientific calculations.\n",
    "from torch.utils.data import DataLoader, Dataset  # data handling.\n",
    "from sklearn.datasets import fetch_openml  # To fetch the hand-written digits dataset.\n",
    "from sklearn.model_selection import train_test_split  # To split the dataset into training and testing sets.\n",
    "import matplotlib.pyplot as plt  # plotting module.\n",
    "from tqdm.notebook import tqdm  # Progress bar\n",
    "from base_model import BaseModel  # base model class.\n",
    "\n",
    "# Type hinting.\n",
    "from torch import Tensor\n",
    "from typing import Optional, Literal, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "# Access features (pixel values). We don't need the target values for autoencoders.\n",
    "dataset = mnist['data']\n",
    "\n",
    "print(f\"The shape of the data array is: {dataset.shape}\")\n",
    "\n",
    "train_set, val_set = train_test_split(dataset, test_size=1/7, random_state=42)\n",
    "train_set, val_set = np.array(train_set), np.array(val_set) \n",
    "\n",
    "print(f\"Training set shape: {train_set.shape}\")\n",
    "print(f\"Validation set shape: {val_set.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class for the auto encoder.\n",
    "class MNISTDataset(Dataset):\n",
    "    __mnist_shape__: Tuple[int, int, int] = (1, 28, 28)\n",
    "\n",
    "    def __init__(self, data: np.ndarray):\n",
    "        self.data = torch.from_numpy(data).to(torch.float32).view(-1, *self.__mnist_shape__) / 255.0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor]:\n",
    "        return self.data[idx], self.data[idx]  # Return the same image as input and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create the dataset loaders.\n",
    "train_dataset = MNISTDataset(train_set)\n",
    "val_dataset = MNISTDataset(val_set)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fully Connected Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAutoEncoder(BaseModel):\n",
    "    \"\"\"Base autoencoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_parms: dict, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_layer_outs: bool = False\n",
    "        self._layer_outs: list = []\n",
    "        self._layer_parms = layer_parms\n",
    "\n",
    "        num_layers = len(list(layer_parms.values())[0])\n",
    "        assert num_layers > 0, \"At least one hidden layer is required.\"\n",
    "\n",
    "        encoder_layers = []\n",
    "        \n",
    "        encoder_layers.extend([self._make_layer(-1, \"encoder\")])\n",
    "        for idx in range(num_layers - 1):\n",
    "            encoder_layers.extend([nn.ReLU(), self._make_layer(idx, \"encoder\")])\n",
    "        encoder_layers.extend([nn.Sigmoid()])\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        decoder_layers = []\n",
    "        for idx in range(num_layers - 1):\n",
    "            decoder_layers.extend([self._make_layer(idx, \"decoder\"), nn.ReLU()])\n",
    "        decoder_layers.extend([self._make_layer(-1, \"decoder\"), nn.Sigmoid()])    \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def _make_layer(self, idx: int, trasnform_type: Literal[\"encoder\", \"decoder\"]) -> nn.Module:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def layer_outs(self) -> list:\n",
    "        self.save_layer_outs = False\n",
    "        _layer_outs = self._layer_outs\n",
    "        self._layer_outs = []\n",
    "        return _layer_outs\n",
    "    \n",
    "    def _attach_hooks(self) -> None:\n",
    "        assert hasattr(self, \"encoder\") and hasattr(self, \"decoder\"), \\\n",
    "            \"Model must have 'encoder' and 'decoder' attributes.\"\n",
    "        for layer in self.encoder:\n",
    "            layer.register_forward_hook(self._hook_fn)\n",
    "        for layer in self.decoder:\n",
    "            layer.register_forward_hook(self._hook_fn)\n",
    "\n",
    "    def _hook_fn(self, module: nn.Module, input: Tuple[Tensor], output: Tensor) -> None:\n",
    "        if self.save_layer_outs and isinstance(module, nn.ReLU):\n",
    "            assert output.size(0) == 1, \"For this operation, only 1 image in a batch is allowed.\"\n",
    "            assert int(output.numel() ** 0.5) ** 2 == output.numel(), \"Only square images are supported.\"\n",
    "            side_len = int(output.numel() ** 0.5)\n",
    "            self._layer_outs.append(output.clone().view(side_len, side_len).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCAutoEncoder(BaseAutoEncoder):\n",
    "    \"\"\"\n",
    "    Fully connected autoencoder model.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size of the input data.\n",
    "        layers (list[int]): The size of each hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        encoder (nn.Sequential): The encoder part of the autoencoder.\n",
    "        decoder (nn.Sequential): The decoder part of the autoencoder.\n",
    "    \"\"\"\n",
    "    __input_size__: int = 28 * 28\n",
    "\n",
    "    def __init__(self, layers: list[int], **kwargs) -> None:\n",
    "        super().__init__({\"layers\": layers}, **kwargs)\n",
    "        self._attach_hooks()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def _make_layer(self, idx: int, trasnform_type: Literal[\"encoder\", \"decoder\"]) -> nn.Module:\n",
    "        if trasnform_type == \"encoder\":\n",
    "            if idx == -1:\n",
    "                return nn.Linear(self.__input_size__, self._layer_parms[\"layers\"][0])\n",
    "            else:\n",
    "                return nn.Linear(self._layer_parms[\"layers\"][idx], self._layer_parms[\"layers\"][idx+1])\n",
    "            \n",
    "        elif trasnform_type == \"decoder\":\n",
    "            if idx == -1:\n",
    "                return nn.Linear(self._layer_parms[\"layers\"][0], self.__input_size__)\n",
    "            else:\n",
    "                return nn.Linear(self._layer_parms[\"layers\"][-idx-1], self._layer_parms[\"layers\"][-idx-2])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid transform type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The FC Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the Auto Encoder, I chose the Mean Squared Error (MSE) loss function, since it gives scores for the pixel similarity between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs for different network architectures.\n",
    "\n",
    "# This is more of a sanity check, it should be able perfectly reconstruct the input\n",
    "#  as the latent space dimension is the same as the input dimension.\n",
    "layers_config1 = [28 * 28]  \n",
    "\n",
    "# This is a decent architecture, with a large enough latent space and few hidden layers.\n",
    "layers_config2 = [400, 400, 64]\n",
    "\n",
    "# This is a very small latent space, it should not be able to reconstruct the input well.\n",
    "layers_config3 = [16, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_autoencoder1 = FCAutoEncoder(layers_config1, job_type=\"regression\")\n",
    "fc_autoencoder1.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "fc_autoencoder1.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_autoencoder2 = FCAutoEncoder(layers_config2, job_type=\"regression\")\n",
    "fc_autoencoder2.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "fc_autoencoder2.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_autoencoder3 = FCAutoEncoder(layers_config3, job_type=\"regression\")\n",
    "fc_autoencoder3.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "fc_autoencoder3.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing The Encoding-Decoding Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encoding_decoding_path(model: BaseAutoEncoder, input_img: Tensor, input_shape: Tuple) -> None:\n",
    "    model.save_layer_outs = True\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_img = model(input_img.view(*input_shape)).view(28, 28).numpy()\n",
    "    layer_outs = [input_img.view(28, 28).numpy(), *model.layer_outs, output_img]\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i in range(len(layer_outs) // 2):\n",
    "        plt.subplot(2, len(layer_outs) // 2, i + 1)\n",
    "        plt.imshow(layer_outs[i], cmap='gray')\n",
    "        plt.title(f\"Layer {i+1} input\")\n",
    "\n",
    "    for i in range(len(layer_outs) // 2, len(layer_outs), 1):\n",
    "        plt.subplot(2, len(layer_outs) // 2, i + 1)\n",
    "        plt.imshow(layer_outs[len(layer_outs) - i + len(layer_outs) // 2 - 1], cmap='gray')\n",
    "        plt.title(f\"Layer {len(layer_outs) - i + len(layer_outs) // 2} output\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc_autoencoder in [fc_autoencoder1, fc_autoencoder2, fc_autoencoder3]:\n",
    "    specific_input = val_dataset[10][0]\n",
    "    plot_encoding_decoding_path(fc_autoencoder, specific_input, input_shape=(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoEncoder(BaseAutoEncoder):\n",
    "    \"\"\"\n",
    "    Fully connected autoencoder model.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The size of the input data.\n",
    "        layers (list[int]): The size of each hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        encoder (nn.Sequential): The encoder part of the autoencoder.\n",
    "        decoder (nn.Sequential): The decoder part of the autoencoder.\n",
    "    \"\"\"\n",
    "    __input_channels__ = 1\n",
    "\n",
    "    def __init__(self, ksizes: list[int], channels: Optional[list[int]] = None, **kwargs) -> None:\n",
    "        super().__init__({\n",
    "            \"channels\": channels if channels is not None else [self.__input_channels__] * len(ksizes), \n",
    "            \"ksizes\": ksizes\n",
    "            }, **kwargs)\n",
    "\n",
    "        if not channels or np.all(np.array(channels) == 1):\n",
    "            self._attach_hooks()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return super().forward(x)\n",
    "    \n",
    "    def _make_layer(self, idx: int, trasnform_type: Literal[\"encoder\", \"decoder\"]) -> nn.Module:\n",
    "        if trasnform_type == \"encoder\":\n",
    "            if idx == -1:\n",
    "                return nn.Conv2d(self.__input_channels__, self._layer_parms[\"channels\"][0], \n",
    "                                 self._layer_parms[\"ksizes\"][0])\n",
    "            \n",
    "            return nn.Conv2d(self._layer_parms[\"channels\"][idx], self._layer_parms[\"channels\"][idx+1], \n",
    "                             self._layer_parms[\"ksizes\"][idx])\n",
    "            \n",
    "        elif trasnform_type == \"decoder\":\n",
    "            if idx == -1:\n",
    "                return nn.ConvTranspose2d(self._layer_parms[\"channels\"][0], self.__input_channels__, \n",
    "                                          self._layer_parms[\"ksizes\"][0])\n",
    "            \n",
    "            return nn.ConvTranspose2d(self._layer_parms[\"channels\"][-idx-1], \n",
    "                                      self._layer_parms[\"channels\"][-idx-2],\n",
    "                                      self._layer_parms[\"ksizes\"][-idx-1])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Invalid transform type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sanity check, the model should be able to perfectly reconstruct the input.\n",
    "ksizes1 = [1]\n",
    "\n",
    "# This is a decent architecture, with a large enough latent space and few hidden layers.\n",
    "ksizes2 = [7, 7]  # Latent space is (28 - 6 - 6)^2 = 256\n",
    "\n",
    "# Good architecture with multiple channels (cannot be displayed).\n",
    "channels3 = [2, 2, 2]; ksizes3 = [7, 7, 7]  # Latent space is (28 - 6 - 6 - 6)^2 * 2 = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder1 = ConvolutionalAutoEncoder(ksizes1, job_type=\"regression\")\n",
    "conv_autoencoder1.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "conv_autoencoder1.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder2 = ConvolutionalAutoEncoder(ksizes2, job_type=\"regression\")\n",
    "conv_autoencoder2.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "conv_autoencoder2.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_autoencoder3 = ConvolutionalAutoEncoder(ksizes3, channels3, job_type=\"regression\")\n",
    "conv_autoencoder3.fit(train_loader, val_loader, num_epochs=15, lr=0.001, print_stride=3)\n",
    "conv_autoencoder3.fit(train_loader, val_loader, num_epochs=10, lr=0.0001, print_stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conv_autoencoder in [conv_autoencoder1, conv_autoencoder2]:\n",
    "    specific_input = val_dataset[10][0]\n",
    "    plot_encoding_decoding_path(conv_autoencoder, specific_input, input_shape=(1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explored the MLP Auto Encoder vs the Fully Convolutional Auto Encoder on the MNIST dataset.\n",
    "\n",
    "We saw that the MLP Auto Encoder performs overall better than the fully convolutional one, probably because the convolutional one lacks fully connected layers, which are crucial for its receptive field.\n",
    "\n",
    "Also, the larger the laten space the network has the better it performs, since it doens't have to encode the image into a smaller space.\n",
    "\n",
    "Choosing the network parameters and the architecture is obviously crucial for the network's performance.\n",
    "\n",
    "We chose MSE as the loss function since it evaluates the model based on the pixel similarity between the original and the reconstructed image, which is exactly the metric we want to optimize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
