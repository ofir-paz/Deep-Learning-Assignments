{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment - Extra Credit #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *22.07.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning* \\\n",
    "***Extra Assignment Course:*** *20999 - Extra Assignment 4*\n",
    "\n",
    "Welcome to the second question of the extra assignment #2 as part of the course *Deep Learning*. \\\n",
    "In this question we will train an auto encoder to denoise a language dataset and afterwards use transfer learning on the trained model for a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import torch.nn.functional as F  # neural network functional module.\n",
    "from torch.utils.data import DataLoader, Dataset  # data handling.\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator  # vocabulary builder.\n",
    "import matplotlib.pyplot as plt  # plotting module.\n",
    "import datasets as ds  # public dataset module.\n",
    "from base_model import BaseModel  # base model class.\n",
    "\n",
    "# Type hinting.\n",
    "from torch import Tensor\n",
    "from torchtext.vocab import Vocab\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add noise to a language dataset, I thought of Four options:\n",
    "1. Make duplicates of random words that appear in the sentence. For example: \n",
    "$$\\text{\"The princess is beautiful\"} \\rightarrow \\text{\"The princess is is beautiful\"}$$\n",
    "2. Add a random word somewhere in the sentence. For example:\n",
    "$$\\text{\"The princess is beautiful\"} \\rightarrow \\text{\"The house princess is beautiful\"}$$\n",
    "3. Changing the order of words in the sentence. For example:\n",
    "$$\\text{\"The princess is beautiful\"} \\rightarrow \\text{\"The beautiful is princess\"}$$\n",
    "4. Changing a random token in the sentence to the unknown token. For example:\n",
    "$$\\text{\"The princess is beautiful\"} \\rightarrow \\text{\"The <unk> is beautiful\"}$$\n",
    "\n",
    "The simplest option and the one that seems like it would work best in language processing is option 4, so I will implement that only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(sentence_tokens: Tensor, vocab: Vocab, noise_term: float = 0.1) -> Tensor:\n",
    "    \"\"\"\n",
    "    Add noise to the sentence tokens.\n",
    "\n",
    "    Args:\n",
    "        sentence_tokens (Tensor): Sentence tokens.\n",
    "        vocab (Vocab): Vocabulary.\n",
    "        noise_term (float): Noise term.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Noisy sentence tokens.\n",
    "    \"\"\"\n",
    "    return sentence_tokens.clone().detach().apply_(lambda token: token if torch.rand(1) > noise_term \n",
    "                                                   else vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Denoiser Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoiserAutoEncoder(BaseModel):\n",
    "    def __init__(self, vocab: Vocab, embed_dim: int, hidden_dim: int, \n",
    "                 encoder_num_layers: int = 1, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = DenoiserEncoder(len(vocab), embed_dim, hidden_dim, encoder_num_layers)\n",
    "        self.decoder = DenoiserDecoder(vocab, embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        if len(tokens.size()) == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "        elif tokens.size(0) > 1:\n",
    "            raise ValueError(\"Can only process one sentence at a time.\")\n",
    "        context = self.encoder(tokens)\n",
    "        output = self.decoder(context, tokens.size(1))\n",
    "        return output\n",
    "\n",
    "\n",
    "class DenoiserEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_layers: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.RNN_stack = nn.LSTM(embed_dim, hidden_dim // 2, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        embedded = self.embedding(tokens)\n",
    "        output, _ = self.RNN_stack(embedded)\n",
    "        context = output[:, -1, :]\n",
    "        return context\n",
    "\n",
    "\n",
    "class DenoiserDecoder(nn.Module):\n",
    "    def __init__(self, vocab: Vocab, embed_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "        self.RNNCell = DecoderRNN(len(vocab), embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, context: Tensor, num_tokens: int) -> Tensor:\n",
    "        self.RNNCell.hidden_state = context\n",
    "        output = []\n",
    "        previous_token = torch.zeros(self.vocab[\"<sos>\"], dtype=torch.long, \n",
    "                                     device=self.embedding.weight.device)  # Default\n",
    "        for _ in range(num_tokens):\n",
    "            embedded_token = self.embedding(previous_token)\n",
    "            logits = self.RNNCell(embedded_token)\n",
    "            previous_token = torch.argmax(logits, dim=1)\n",
    "            output.append(logits.squeeze(0))\n",
    "        return torch.stack(output)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_state = torch.zeros(1, hidden_dim)  # Assume batch size of 1.\n",
    "        self.RNN_cell = nn.RNNCell(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, embedded_token: Tensor) -> Tensor:\n",
    "        self.hidden_state = self.RNN_cell(embedded_token, self.hidden_state)\n",
    "        output = self.fc(self.hidden_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset to try to fit on.\n",
    "full_dataset: ds.DatasetDict = ds.load_dataset(\"glue\", \"sst2\", keep_in_memory=True)  # type: ignore\n",
    "\n",
    "big_train_dataset = full_dataset[\"train\"]\n",
    "big_validation_dataset = full_dataset[\"validation\"]\n",
    "train_dataset = big_train_dataset.select(range(500))  # small dataset for testing.\n",
    "validation_dataset = big_validation_dataset.select(range(100))  # small dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary.\n",
    "train_sentence_list = train_dataset[\"sentence\"]\n",
    "vocab = build_vocab_from_iterator(map(str.split, train_sentence_list), specials=[\"<unk>\", \"<sos>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after loading the datasets and creating the vocabulary we can show examples of adding noise to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \"the package in which this fascinating -- and timely -- content comes wrapped is disappointingly generic . \"\n",
      "Noisy sentence: \"the package <unk> which this fascinating -- and timely -- content comes wrapped is disappointingly generic .\"\n"
     ]
    }
   ],
   "source": [
    "def get_random_normal_and_noisy_sentence(dataset: ds.Dataset, vocab: Vocab) -> Tuple[str, str]:\n",
    "    random_sentence = random.choice(dataset)[\"sentence\"]\n",
    "    random_sentence_tokens = torch.tensor(vocab(random_sentence.split()), dtype=torch.long)\n",
    "    random_sentence_noisy_tokens = list(add_noise(random_sentence_tokens, vocab))\n",
    "    random_sentence_noisy = \" \".join(vocab.lookup_tokens(random_sentence_noisy_tokens))\n",
    "\n",
    "    return random_sentence, random_sentence_noisy\n",
    "\n",
    "random_sentence, random_sentence_noisy = get_random_normal_and_noisy_sentence(train_dataset, vocab)\n",
    "print(f\"Original sentence: \\\"{random_sentence}\\\"\")\n",
    "print(f\"Noisy sentence: \\\"{random_sentence_noisy}\\\"\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SST-2 dataset.\n",
    "class NoisySST2Dataset(Dataset):\n",
    "    def __init__(self, dataset: ds.Dataset, vocab: Vocab, noise_term: float = 0.1) -> None:\n",
    "        self.sentences = list(map(lambda seq: torch.tensor(vocab(seq.split()), dtype=torch.long), \n",
    "                                  dataset[\"sentence\"]))\n",
    "        self.vocab = vocab\n",
    "        self.noise_term = noise_term\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        return add_noise(self.sentences[idx], self.vocab, self.noise_term), self.sentences[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders.\n",
    "train_set = NoisySST2Dataset(train_dataset, vocab)\n",
    "validation_set = NoisySST2Dataset(validation_dataset, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for training.\n",
      "[epoch: 01/07] [Train loss: 7.176482  Train Accuracy: 0.039]  [Val loss: 7.165845  Val Accuracy: 0.042]\n",
      "[epoch: 02/07] [Train loss: 6.432540  Train Accuracy: 0.055]  [Val loss: 7.144907  Val Accuracy: 0.042]\n",
      "[epoch: 03/07] [Train loss: 6.054338  Train Accuracy: 0.063]  [Val loss: 7.339632  Val Accuracy: 0.058]\n",
      "[epoch: 04/07] [Train loss: 5.726097  Train Accuracy: 0.078]  [Val loss: 7.497063  Val Accuracy: 0.054]\n",
      "[epoch: 05/07] [Train loss: 5.408067  Train Accuracy: 0.094]  [Val loss: 7.604751  Val Accuracy: 0.077]\n",
      "[epoch: 06/07] [Train loss: 5.106171  Train Accuracy: 0.118]  [Val loss: 7.662863  Val Accuracy: 0.077]\n",
      "[epoch: 07/07] [Train loss: 4.809315  Train Accuracy: 0.147]  [Val loss: 7.881088  Val Accuracy: 0.064]\n",
      "Using CUDA for training.\n",
      "[epoch: 08/13] [Train loss: 4.343102  Train Accuracy: 0.212]  [Val loss: 7.638634  Val Accuracy: 0.075]\n",
      "[epoch: 09/13] [Train loss: 4.223228  Train Accuracy: 0.227]  [Val loss: 7.681630  Val Accuracy: 0.077]\n",
      "[epoch: 10/13] [Train loss: 4.119379  Train Accuracy: 0.253]  [Val loss: 7.749040  Val Accuracy: 0.084]\n",
      "[epoch: 11/13] [Train loss: 4.079857  Train Accuracy: 0.263]  [Val loss: 7.778761  Val Accuracy: 0.087]\n",
      "[epoch: 12/13] [Train loss: 4.018014  Train Accuracy: 0.275]  [Val loss: 7.858726  Val Accuracy: 0.074]\n",
      "[epoch: 13/13] [Train loss: 3.995585  Train Accuracy: 0.272]  [Val loss: 7.919746  Val Accuracy: 0.075]\n"
     ]
    }
   ],
   "source": [
    "# Create the model and train the denoiser autoencoder.\n",
    "denoiser = DenoiserAutoEncoder(vocab, 64, 128, 1, job_type=\"single-sentence-autoencoder\")\n",
    "denoiser.fit(train_loader, validation_loader, num_epochs=7, lr=0.001)\n",
    "denoiser.fit(train_loader, validation_loader, num_epochs=6, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "\t\"remains utterly satisfied to remain the same throughout\"\n",
      "\n",
      "Noisy sentence:\n",
      "\t\"remains <unk> satisfied to remain the same throughout\"\n",
      "\n",
      "Denoised sentence:\n",
      "\t\"ugly n't and to and . , .\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of using the model.\n",
    "noisy_sentence, sentence = train_set[3]\n",
    "\n",
    "# Denoise the sentence.\n",
    "with torch.no_grad():\n",
    "    denoiser.eval()\n",
    "    denoised_sentence_logits = denoiser(noisy_sentence.unsqueeze(0))\n",
    "    denoised_sentence = torch.argmax(denoised_sentence_logits, dim=1)\n",
    "denoiser.train()\n",
    "\n",
    "string_sentence = \" \".join(vocab.lookup_tokens(list(sentence)))\n",
    "string_noisy_sentence = \" \".join(vocab.lookup_tokens(list(noisy_sentence)))\n",
    "string_denoised_sentence = \" \".join(vocab.lookup_tokens(list(denoised_sentence)))\n",
    "\n",
    "# Print the sentences.\n",
    "print(f\"Original sentence:\\n\\t\\\"{string_sentence}\\\"\\n\")\n",
    "print(f\"Noisy sentence:\\n\\t\\\"{string_noisy_sentence}\\\"\\n\")\n",
    "print(f\"Denoised sentence:\\n\\t\\\"{string_denoised_sentence}\\\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the trained model for a classification task. We will take the encoder part of the auto-denoiser, and we will add a linear layer on top of the encoder to classify the data.\n",
    "We are basically going to classify from the encoder's representation of the data that was learned during the denoising task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification dataset.\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, dataset: ds.Dataset, vocab: Vocab) -> None:\n",
    "        self.sentences = list(map(lambda seq: torch.tensor(vocab(seq.split())), dataset[\"sentence\"]))\n",
    "        self.labels = torch.tensor(dataset[\"label\"], dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_train_set = SST2Dataset(train_dataset, vocab)\n",
    "classification_validation_set = SST2Dataset(validation_dataset, vocab)\n",
    "classification_train_loader = DataLoader(classification_train_set, batch_size=1, shuffle=True)\n",
    "classification_validation_loader = DataLoader(classification_validation_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification model.\n",
    "class TransferClassifer(BaseModel):\n",
    "    def __init__(self, encoder: nn.Module, hidden_dim: int, num_classes: int, **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, tokens: Tensor) -> Tensor:\n",
    "        if len(tokens.size()) == 1:\n",
    "            tokens = tokens.unsqueeze(0)\n",
    "        elif tokens.size(0) > 1:\n",
    "            raise ValueError(\"Can only process one sentence at a time.\")\n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            context = self.encoder(tokens)\n",
    "        output = self.fc(context)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for training.\n",
      "[epoch: 01/10] [Train loss: 0.733546  Train Accuracy: 0.480]  [Val loss: 0.762116  Val Accuracy: 0.520]\n",
      "[epoch: 02/10] [Train loss: 0.696875  Train Accuracy: 0.576]  [Val loss: 0.845932  Val Accuracy: 0.520]\n",
      "[epoch: 03/10] [Train loss: 0.677201  Train Accuracy: 0.556]  [Val loss: 0.740179  Val Accuracy: 0.490]\n",
      "[epoch: 04/10] [Train loss: 0.668494  Train Accuracy: 0.612]  [Val loss: 0.888941  Val Accuracy: 0.490]\n",
      "[epoch: 05/10] [Train loss: 0.660632  Train Accuracy: 0.588]  [Val loss: 0.744503  Val Accuracy: 0.490]\n",
      "[epoch: 06/10] [Train loss: 0.647558  Train Accuracy: 0.632]  [Val loss: 0.740986  Val Accuracy: 0.450]\n",
      "[epoch: 07/10] [Train loss: 0.647347  Train Accuracy: 0.626]  [Val loss: 0.723115  Val Accuracy: 0.500]\n",
      "[epoch: 08/10] [Train loss: 0.638600  Train Accuracy: 0.640]  [Val loss: 0.751004  Val Accuracy: 0.510]\n",
      "[epoch: 09/10] [Train loss: 0.630565  Train Accuracy: 0.656]  [Val loss: 0.834213  Val Accuracy: 0.480]\n",
      "[epoch: 10/10] [Train loss: 0.635711  Train Accuracy: 0.658]  [Val loss: 0.748020  Val Accuracy: 0.500]\n",
      "Using CPU for training.\n",
      "[epoch: 11/15] [Train loss: 0.594026  Train Accuracy: 0.692]  [Val loss: 0.731747  Val Accuracy: 0.490]\n",
      "[epoch: 12/15] [Train loss: 0.593369  Train Accuracy: 0.690]  [Val loss: 0.727730  Val Accuracy: 0.470]\n",
      "[epoch: 13/15] [Train loss: 0.591096  Train Accuracy: 0.700]  [Val loss: 0.730185  Val Accuracy: 0.470]\n",
      "[epoch: 14/15] [Train loss: 0.589081  Train Accuracy: 0.692]  [Val loss: 0.730390  Val Accuracy: 0.470]\n",
      "[epoch: 15/15] [Train loss: 0.589671  Train Accuracy: 0.704]  [Val loss: 0.731849  Val Accuracy: 0.500]\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model.\n",
    "transfer_classifer = TransferClassifer(denoiser.encoder, 128, 2, job_type=\"classification\")\n",
    "transfer_classifer.fit(classification_train_loader, classification_validation_loader, \n",
    "                         num_epochs=10, lr=0.0025, wd=0.0001, try_cuda=False)\n",
    "transfer_classifer.fit(classification_train_loader, classification_validation_loader, \n",
    "                         num_epochs=5, lr=0.0005, wd=0.0001, try_cuda=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
