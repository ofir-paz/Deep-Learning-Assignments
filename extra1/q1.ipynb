{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment - Extra Credit #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *15.07.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning* \\\n",
    "***Extra Assignment Course:*** *20998 - Extra Assignment 3*\n",
    "\n",
    "Welcome to the first question of the extra assignment #1 as part of the course *Deep Learning*. \\\n",
    "In this question we will train an RNN network for classification on the SST-2 dataset while dealing with the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import torch.optim as optim  # optimization module.\n",
    "import torch.nn.functional as F  # functional module.\n",
    "import numpy as np  # numpy.\n",
    "from torch.utils.data import DataLoader, Dataset  # data handling.\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator  # vocabulary builder.\n",
    "import matplotlib.pyplot as plt  # plotting module.\n",
    "import datasets as ds  # public dataset module.\n",
    "from base_model import BaseModel  # base model class.\n",
    "\n",
    "# Type hinting.\n",
    "from torch import Tensor\n",
    "from torchtext.vocab import Vocab\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset.\n",
    "dataset: ds.DatasetDict = ds.load_dataset(\"glue\", \"sst2\")  # type: ignore\n",
    "\n",
    "train_set = dataset[\"train\"][:1500]\n",
    "validation_set = dataset[\"validation\"][:500]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary.\n",
    "vocab = build_vocab_from_iterator(map(str.split, train_set[\"sentence\"]), specials=[\"<unk>\"], min_freq=5)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SST-2 dataset.\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, dataset: ds.Dataset, vocab: Vocab) -> None:\n",
    "        self.sentences = list(map(lambda seq: torch.tensor(vocab(seq.split())), dataset[\"sentence\"]))\n",
    "        self.labels = torch.tensor(dataset[\"label\"], dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SST2Dataset(train_set, vocab)\n",
    "validation_dataset = SST2Dataset(validation_set, vocab)\n",
    "test_dataset = SST2Dataset(test_set, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClasifer(BaseModel):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network (RNN) classifier, designed specifically for the SST-2 dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int,\n",
    "                 **kwargs) -> None:\n",
    "        super(RNNClasifer, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_cell = nn.RNNCell(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.saved_grads = []\n",
    "        \n",
    "        # Define t, the maximum time steps that the gradient will be \n",
    "        # backpropagated through in the RNN.\n",
    "        self.t = 10_000\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, seq_len)\n",
    "        t: token index at which to stop gradient propagation.\n",
    "                Gradients will only be propagated from tokens at or after this index.\n",
    "        \"\"\"\n",
    "        self.saved_grads = []  # Reset the saved gradients.\n",
    "        batch_size, seq_len = x.size()\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        h_t = torch.zeros(batch_size, self.rnn_cell.hidden_size, device=x.device)\n",
    "\n",
    "        # Unroll the recurrence manually\n",
    "        for i in range(seq_len):\n",
    "            # Process the i-th token.\n",
    "            h_t = self.rnn_cell(embedded[:, i, :], h_t)\n",
    "\n",
    "            if self.training:\n",
    "                # (Optional) Register a hook to observe gradient flow.\n",
    "                h_t.register_hook(lambda grad: self.saved_grads.append(grad.norm().item()))\n",
    "                # Detach the hidden state once we hit the t token.\n",
    "                if i == self.t:\n",
    "                    h_t = h_t.detach()\n",
    "            if i >= self.t:\n",
    "                self.saved_grads.append(0.0)  # Pad the gradient list.\n",
    "        logits = self.fc(h_t)\n",
    "        return logits\n",
    "    \n",
    "    def plot_grads(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the gradient norms for each token in the sequence.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.saved_grads, marker='o', linestyle='-', color='b', label='Gradient Norm')\n",
    "        plt.xlabel(\"Token Index\", fontsize=12)\n",
    "        plt.ylabel(\"Gradient Norm\", fontsize=12)\n",
    "        plt.title(\"Gradient Norms for Each Token in the Sequence\", fontsize=14)\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define t, the maximum time steps that the gradient will be \n",
    "# backpropagated through in the RNN.\n",
    "t = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "model = RNNClasifer(len(vocab), embed_dim=32, hidden_dim=64, num_classes=2, task_type=\"classification\")\n",
    "model.t = t\n",
    "\n",
    "# Dummy input: batch_size = 1, seq_len = 10\n",
    "x = torch.randint(0, len(vocab), (1, 10))\n",
    "print(x.shape)\n",
    "\n",
    "logits = model(x)\n",
    "loss = F.cross_entropy(logits, torch.tensor([0]))\n",
    "loss.backward()\n",
    "model.plot_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. with t=1000, the gradient will be backpropagated through all tokens.\n",
    "model1 = RNNClasifer(len(vocab), embed_dim=32, hidden_dim=64, num_classes=2, task_type=\"classification\")\n",
    "model1.t = 1000\n",
    "model1.fit(train_loader, validation_loader, num_epochs=3, try_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. with t=10\n",
    "model2 = RNNClasifer(len(vocab), embed_dim=32, hidden_dim=64, num_classes=2, task_type=\"classification\")\n",
    "model2.t = 10\n",
    "model2.fit(train_loader, validation_loader, num_epochs=3, try_cuda=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
