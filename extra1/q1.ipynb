{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment - Extra Credit #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *15.07.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning* \\\n",
    "***Extra Assignment Course:*** *20998 - Extra Assignment 3*\n",
    "\n",
    "Welcome to the first question of the extra assignment #1 as part of the course *Deep Learning*. \\\n",
    "In this question we will train an RNN network for classification on the SST-2 dataset while dealing with the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import torch.optim as optim  # optimization module.\n",
    "import torch.nn.functional as F  # functional module.\n",
    "import numpy as np  # numpy.\n",
    "from torch.utils.data import DataLoader, Dataset  # data handling.\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator  # vocabulary builder.\n",
    "import matplotlib.pyplot as plt  # plotting module.\n",
    "import datasets as ds  # public dataset module.\n",
    "from base_model import BaseModel  # base model class.\n",
    "\n",
    "# Type hinting.\n",
    "from torch import Tensor\n",
    "from torchtext.vocab import Vocab\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SST-2 dataset.\n",
    "dataset: ds.DatasetDict = ds.load_dataset(\"glue\", \"sst2\")  # type: ignore\n",
    "\n",
    "train_set = dataset[\"train\"][:3000]\n",
    "validation_set = dataset[\"validation\"][:1000]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary.\n",
    "vocab = build_vocab_from_iterator(map(str.split, train_set[\"sentence\"]), specials=[\"<unk>\"], min_freq=5)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SST-2 dataset.\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, dataset: ds.Dataset, vocab: Vocab) -> None:\n",
    "        self.sentences = list(map(lambda seq: torch.tensor(vocab(seq.split())), dataset[\"sentence\"]))\n",
    "        self.labels = torch.tensor(dataset[\"label\"], dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SST2Dataset(train_set, vocab)\n",
    "validation_dataset = SST2Dataset(validation_set, vocab)\n",
    "test_dataset = SST2Dataset(test_set, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClasifer(BaseModel):\n",
    "    \"\"\"\n",
    "    Recurrent Neural Network (RNN) classifier, designed specifically for the SST-2 dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocab, t: int, embed_dim: int, hidden_dim: int, num_classes: int,\n",
    "                 RNNlayers: int = 2, **kwargs) -> None:\n",
    "        super(RNNClasifer, self).__init__(**kwargs)\n",
    "        self.t = t\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_dim)\n",
    "        self.rnns = nn.RNN(embed_dim, hidden_dim, RNNlayers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def rnn_forward(self, x: Tensor) -> Tensor:\n",
    "        x, _ = self.rnns(x)\n",
    "        return x\n",
    "    \n",
    "    def normal_forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = self.rnn_forward(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: Tensor, capture_gradients: bool = False) -> Tensor:\n",
    "        assert len(x.size()) == 2 and x.size(0) == 1, f\"Can only process batch size of 1. Got: {x.size()}.\"\n",
    "        if x.size(1) <= self.t:\n",
    "            return self.normal_forward(x)\n",
    "        \n",
    "        x = self.embedding(x)  # Shape: (batch_size, seq_len, embed_dim).\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient tracking for the first T-t tokens.\n",
    "            x_no_grad = self.rnn_forward(x[:, :-self.t])  # Shape: (T-t, hidden_dim).\n",
    "        \n",
    "\n",
    "        # Last t tokens (with gradient tracking).\n",
    "        x_grad = self.rnn_forward(x[:, -self.t:])  # Shape: (t, hidden_dim).\n",
    "\n",
    "        # If we want to capture gradients, register hooks to save gradient norms.\n",
    "        if capture_gradients:\n",
    "            self.save_grads(x_grad)\n",
    "\n",
    "        # Combine the outputs.\n",
    "        combined_output = torch.cat((x_no_grad, x_grad), dim=1)  # Shape: (batch_size, seq_len, hidden_dim).\n",
    "        \n",
    "        # Take the hidden state of the last token.\n",
    "        final_output = combined_output[:, -1, :]  # Shape: (batch_size, hidden_dim).\n",
    "\n",
    "        # Pass through the fully connected layer for classification.\n",
    "        final_output = self.fc(final_output)  # Shape: (batch_size, num_classes).\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def save_grads(self, rnn_outs: Tensor) -> None:\n",
    "        # Compute the norm of the gradient and store it\n",
    "        self.gradient_norms = []\n",
    "        for i in range(rnn_outs.size(1)):  # Iterate over the sequence length (last t tokens)\n",
    "            grad_norm = rnn_outs[:, i].backward(retain_graph=True).norm().item()\n",
    "            self.gradient_norms.append(grad_norm)\n",
    "    \n",
    "    def plot_gradients(self) -> None:\n",
    "        \"\"\"\n",
    "        Plots the gradient norms for each token after backpropagation.\n",
    "        \"\"\"\n",
    "        if len(self.gradient_norms) == 0:\n",
    "            print(\"No gradients captured yet.\")\n",
    "            return\n",
    "\n",
    "        # Plot the gradient norms\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(-self.t, 0), self.gradient_norms, marker='o', label=\"Gradient Norms (Last t Tokens)\")\n",
    "        plt.axhline(0, color='r', linestyle='--', label='No Gradient (First T-t Tokens)')\n",
    "        plt.xlabel(\"Token Index (Relative to Last t Tokens)\")\n",
    "        plt.ylabel(\"Gradient Norm\")\n",
    "        plt.title(\"Gradient Norms for the Last t Tokens\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for training.\n",
      "[epoch: 01/10] [Train loss: 0.699764  Train Accuracy: 0.532]  [Val loss: 0.694710]  Val Accuracy: 0.513]\n",
      "[epoch: 02/10] [Train loss: 0.679523  Train Accuracy: 0.578]  [Val loss: 0.695728]  Val Accuracy: 0.526]\n",
      "[epoch: 03/10] [Train loss: 0.645737  Train Accuracy: 0.620]  [Val loss: 0.708480]  Val Accuracy: 0.537]\n",
      "[epoch: 04/10] [Train loss: 0.604211  Train Accuracy: 0.662]  [Val loss: 0.704215]  Val Accuracy: 0.561]\n",
      "[epoch: 05/10] [Train loss: 0.556997  Train Accuracy: 0.699]  [Val loss: 0.698462]  Val Accuracy: 0.544]\n",
      "[epoch: 06/10] [Train loss: 0.507165  Train Accuracy: 0.739]  [Val loss: 0.777916]  Val Accuracy: 0.547]\n",
      "[epoch: 07/10] [Train loss: 0.469323  Train Accuracy: 0.761]  [Val loss: 0.792984]  Val Accuracy: 0.547]\n",
      "[epoch: 08/10] [Train loss: 0.422144  Train Accuracy: 0.801]  [Val loss: 0.823008]  Val Accuracy: 0.549]\n",
      "[epoch: 09/10] [Train loss: 0.389506  Train Accuracy: 0.811]  [Val loss: 0.842547]  Val Accuracy: 0.568]\n",
      "[epoch: 10/10] [Train loss: 0.351911  Train Accuracy: 0.824]  [Val loss: 0.829034]  Val Accuracy: 0.555]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "model = RNNClasifer(vocab, t=5, embed_dim=32, hidden_dim=64, num_classes=2, RNNlayers=1, \n",
    "                    task_type=\"classification\")\n",
    "# Train the model.\n",
    "model.fit(train_loader, validation_loader, num_epochs=2, try_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot the gradient norms\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mplot_gradients()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 40\u001b[0m, in \u001b[0;36mRNNClasifer.forward\u001b[1;34m(self, x, capture_gradients)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# If we want to capture gradients, register hooks to save gradient norms.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capture_gradients:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Combine the outputs.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m combined_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x_no_grad, x_grad), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, seq_len, hidden_dim).\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 57\u001b[0m, in \u001b[0;36mRNNClasifer.save_grads\u001b[1;34m(self, rnn_outs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_norms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rnn_outs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# Iterate over the sequence length (last t tokens)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_outs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnorm()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_norms\u001b[38;5;241m.\u001b[39mappend(grad_norm)\n",
      "File \u001b[1;32md:\\Programming\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    252\u001b[0m     (inputs,)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    257\u001b[0m )\n\u001b[0;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32md:\\Programming\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:133\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         )\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[0;32m    137\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "model = RNNClasifer(vocab, t=5, embed_dim=32, hidden_dim=64, num_classes=2, RNNlayers=1, \n",
    "                    task_type=\"classification\")\n",
    "# Plot the gradient norms\n",
    "for x, y in train_loader:\n",
    "    model(x, capture_gradients=True)\n",
    "    model.plot_gradients()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
