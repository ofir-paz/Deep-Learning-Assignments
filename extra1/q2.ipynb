{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher's Assignment - Extra Credit #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Author:*** *Ofir Paz* $\\qquad$ ***Version:*** *15.07.2024* $\\qquad$ ***Course:*** *22961 - Deep Learning* \\\n",
    "***Extra Assignment Course:*** *20998 - Extra Assignment 3*\n",
    "\n",
    "Welcome to the second question of the extra assignment #1 as part of the course *Deep Learning*. \\\n",
    "In this question we will implement an RNN block with a basic pass-through control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # pytorch.\n",
    "import torch.nn as nn  # neural network module.\n",
    "import torch.nn.functional as F  # neural network functional module.\n",
    "from torch.utils.data import DataLoader, Dataset  # data handling.\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.vocab import build_vocab_from_iterator  # vocabulary builder.\n",
    "import matplotlib.pyplot as plt  # plotting module.\n",
    "import datasets as ds  # public dataset module.\n",
    "from base_model import BaseModel  # base model class.\n",
    "\n",
    "# Type hinting.\n",
    "from torch import Tensor\n",
    "from torchtext.vocab import Vocab\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom RNN cell class.\n",
    "\n",
    "    Use another hidden state - the pass-through hidden state to control the hidden state of the RNN cell.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int) -> None:\n",
    "        super(CustomRNNCell, self).__init__()\n",
    "        self.input_linear = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.hidden_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.regular_activation = nn.Tanh()\n",
    "\n",
    "        self.pass_through_layer = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.pass_through_activation = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, one_embedded_token: Tensor, hidden_state: Tensor) -> Tensor:\n",
    "        Z1 = self.input_linear(one_embedded_token)\n",
    "        Z2 = self.hidden_linear(hidden_state)\n",
    "        h_hat_t = self.regular_activation(Z1 + Z2)\n",
    "        \n",
    "        r_t = self.pass_through_activation(self.pass_through_layer(one_embedded_token))\n",
    "        new_hidden_state = h_hat_t * r_t\n",
    "\n",
    "        return new_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement \"forgetness\" by multiplying the data we want to \"forget\" by a factor that is smaller than 1, thus we are lowering the signal and \"forget\" it. This could be beneficial since we might want to pass through only part of the signal, in some cases where the other part is irrelevant to the rest of the text or is made up of noise. \\\n",
    "In this question, we are multiplying the signal by the same signal that is created with the current token only while augmenting it with learned weights.\n",
    "\n",
    "I chose the softmax activation function for $R_t$, since this function has the range $(0, 1)$ so $R_t$ can only lower the signal and not increase it.\n",
    "\n",
    "In my opinion, we can also implement forgetness by implmenting an exponential decay with the previous signal (just multiply the hidden state with some $0<r<1$). In this way we use less parameters and the signal will be forgotten in a more natural way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing The Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset to try to fit on.\n",
    "full_dataset: ds.DatasetDict = ds.load_dataset(\"glue\", \"sst2\")  # type: ignore\n",
    "big_train_dataset = full_dataset[\"train\"]\n",
    "big_validation_dataset = full_dataset[\"validation\"]\n",
    "train_dataset = big_train_dataset.select(range(500))  # small dataset for testing.\n",
    "validation_dataset = big_validation_dataset.select(range(250))  # small dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary.\n",
    "train_sentence_list = train_dataset[\"sentence\"]\n",
    "vocab = build_vocab_from_iterator(map(str.split, train_sentence_list), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset class.\n",
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, dataset: ds.Dataset, vocab: Vocab) -> None:\n",
    "        self.sentences = list(map(lambda seq: torch.tensor(vocab(seq.split())), dataset[\"sentence\"]))\n",
    "        self.labels = torch.tensor(dataset[\"label\"], dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, Tensor]:\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloaders.\n",
    "train_set = SST2Dataset(train_dataset, vocab)\n",
    "validation_set = SST2Dataset(validation_dataset, vocab)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(BaseModel):\n",
    "    \"\"\"\n",
    "    RNN model class.\n",
    "\n",
    "    The RNN model class uses the custom RNN cell to create a custom RNN model.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int, num_classes: int) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn_cell = CustomRNNCell(embed_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, sentence_tokens: Tensor) -> Tensor:\n",
    "        batch_size = sentence_tokens.size(0)\n",
    "        hidden_state = torch.zeros(batch_size, self.rnn_cell.hidden_linear.out_features, \n",
    "                                   device=sentence_tokens.device)\n",
    "        embedded = self.embed(sentence_tokens)\n",
    "        \n",
    "        for i in range(embedded.size(1)):\n",
    "            hidden_state = self.rnn_cell(embedded[:, i, :], hidden_state)\n",
    "        \n",
    "        return self.fc(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for training.\n",
      "[epoch: 01/15] [Train loss: 0.693001  Train Accuracy: 0.510]  [Val loss: 0.692427]  Val Accuracy: 0.516]\n",
      "[epoch: 04/15] [Train loss: 0.661650  Train Accuracy: 0.702]  [Val loss: 0.704553]  Val Accuracy: 0.520]\n",
      "[epoch: 07/15] [Train loss: 0.503337  Train Accuracy: 0.802]  [Val loss: 0.698057]  Val Accuracy: 0.524]\n",
      "[epoch: 10/15] [Train loss: 0.309917  Train Accuracy: 0.908]  [Val loss: 0.770862]  Val Accuracy: 0.520]\n",
      "[epoch: 13/15] [Train loss: 0.172037  Train Accuracy: 0.958]  [Val loss: 1.052298]  Val Accuracy: 0.516]\n",
      "[epoch: 15/15] [Train loss: 0.114313  Train Accuracy: 0.980]  [Val loss: 1.105240]  Val Accuracy: 0.540]\n",
      "Using CPU for training.\n",
      "[epoch: 16/25] [Train loss: 0.091820  Train Accuracy: 0.984]  [Val loss: 1.133645]  Val Accuracy: 0.536]\n",
      "[epoch: 18/25] [Train loss: 0.086085  Train Accuracy: 0.986]  [Val loss: 1.176048]  Val Accuracy: 0.544]\n",
      "[epoch: 20/25] [Train loss: 0.081168  Train Accuracy: 0.988]  [Val loss: 1.206758]  Val Accuracy: 0.544]\n",
      "[epoch: 22/25] [Train loss: 0.076630  Train Accuracy: 0.988]  [Val loss: 1.226948]  Val Accuracy: 0.544]\n",
      "[epoch: 24/25] [Train loss: 0.072347  Train Accuracy: 0.988]  [Val loss: 1.253175]  Val Accuracy: 0.540]\n",
      "[epoch: 25/25] [Train loss: 0.070320  Train Accuracy: 0.988]  [Val loss: 1.266514]  Val Accuracy: 0.540]\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "model = RNN(len(vocab), embed_dim=20, hidden_dim=100, num_classes=2)\n",
    "\n",
    "model.fit(train_loader, validation_loader, num_epochs=15, lr=0.001, try_cuda=False, print_stride=3)\n",
    "model.fit(train_loader, validation_loader, num_epochs=10, lr=0.0001, try_cuda=False, print_stride=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
